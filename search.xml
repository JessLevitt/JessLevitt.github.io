<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Triplet Loss and Online Triplet Mining</title>
      <link href="/2021/08/14/Triplet%20Loss%20and%20Online%20Triplet%20Mining/"/>
      <url>/2021/08/14/Triplet%20Loss%20and%20Online%20Triplet%20Mining/</url>
      
        <content type="html"><![CDATA[<h2 id="Background">Background</h2><p>在论文<a href="https://arxiv.org/abs/1503.03832">FaceNet: A Unified Embedding for Face Recognition and Clustering</a>中，triplet loss被首次提出并用于来学习人脸embedding。</p><p>在一般的classification task中，样本的类别数是事先固定的，我们可以使用softmax corss entropy loss训练network。然而，在某些情况下类别数是一个变量。以face recognition为例，我们需要对比两张face image来判断他们是否是同一个人，然而其中任意一张image都可能未在training set中出现。因而，我们希望能够训练这样一个network：在其对应的embedding space中，同类样本紧密相邻，异类样本形成分离良好的簇。</p><h2 id="Definition-of-Triplet-Loss">Definition of Triplet Loss</h2><p><img src="https://cdn.jsdelivr.net/gh/JessLevitt/CDN/manifest/blog/20210814151303.png" alt=""></p><p>Triplet loss的思想十分简单，它希望在embedding space中：</p><ul><li>同类的两个样本对应的embeddings尽可能地相近</li><li>异类的两个样本对应的embeddings尽可能地远离</li></ul><p>但是，我们其实并不“强求”同类所有样本的embedding都collapse到一个非常小的簇中。而只是希望给定两个同类样本(positive examples)和一个异类样本(negative example)，negative example到最近的positive example的距离要大于一个margin，这一思想和SVM十分类似。</p><p>正式地，triplet loss定义在一个embedding triplet上：</p><ul><li>一个anchor</li><li>一个positive(与anchor同类)</li><li>一个negative(与anchor异类)</li></ul><p>对于某种在embedding space上距离度量方法 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span>，在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(a,p,n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>上的triplet loss定义为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>−</mo><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="normal">margin</mi><mo>⁡</mo><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}=\max (d(a, p)-d(a, n)+\operatorname{margin}, 0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathcal">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">margin</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p><p>我们通过最小化triplet loss来使得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(a,n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span> ​大于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>i</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">d(a,p) + margin</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">in</span></span></span></span>​​。</p><h2 id="Triplet-Mining">Triplet Mining</h2><p>基于triplet loss的定义，会出现三种不同类型的triplets:</p><ul><li>easy triplets: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>i</mi><mi>n</mi><mo>&lt;</mo><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(a,p) + margin &lt; d(a,n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">in</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>，此时triplet loss为零</li><li>hard triplets: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(a,n)&lt;d(a,p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span>​​，此时negative比positive更靠近anchor</li><li>semi-hard triplets: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>i</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">d(a, p)&lt;d(a, n)&lt;d(a, p)+ margin</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">in</span></span></span></span>，此时positive更靠近anchor，但是negative与positive的距离小于margin</li></ul><p><img src="https://cdn.jsdelivr.net/gh/JessLevitt/CDN/manifest/blog/20210814144658.png" alt=""></p><p>选择不同的triplets训练会很大程度地影响最终network的generalization performance。在Facenet论文中，给定anchor和positive，作者随机选择一个semi-hard negative组成triplet训练network。</p><h3 id="Offline-and-Online-Triplet-Mining">Offline and Online Triplet Mining</h3><p>我们已经定义了triplet loss以及不同种类的triplets，接下来的问题就是如果mining这些用于训练的triplets。</p><h4 id="Offline-Triplet-Mining">Offline Triplet Mining</h4><p>在每个training epoch开始之前，计算整个training set的embeddings，然后根据策略选择hard triplets或者semi-hard triplets，然后只在这些triplets上训练network。这种方式显然是非常低效的。</p><h4 id="Online-Triplet-Mining">Online Triplet Mining</h4><p>在Facenet论文中使用的是online triplet mining方法：对于每个batch data，动态地生成用于训练的triplets。假设一个batch包含了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 个样本，那么我们最多可以找到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>B</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">B^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>​ 个triplets。当然其中很多triplets都是invalid的(不满足两个positives和一个negative)。online triplet mining可以高效地识别出valid triplets。</p><h2 id="Strategies-in-Online-Mining">Strategies in Online Mining</h2><p>在online triplet mining中，我们首先计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>​ 个样本的embedding，然后再生成valid triplets。假设挑选的三个样本对应的索引为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">i,j,k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>。那么当 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathnormal">i</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> 属于同类，而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 是异类时，这样的triplet是valid。</p><p>假设一个batch中的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 个样本，对应 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> 不同的人，每个人有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>​ 张face images。那么，可以有两种不同的mining strategies:</p><ul><li>batch all: 识别所有的valid triplets，仅计算在hard triplets和semi-hard triplets上的average loss。<ul><li>关键点在于不将easy triplets的loss计算在内，否则average loss会非常小</li><li>valid triplets的数量是：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>K</mi><mo stretchy="false">(</mo><mi>K</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>P</mi><mi>K</mi><mo>−</mo><mi>K</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P K(K-1)(P K-K)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mclose">)</span></span></span></span></li></ul></li><li>batch hard: 对于每一个anchor，选择hardest positive(具有最大的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(a,p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span>​)和hardest negatvie(具有最小的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(a,n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>)。<ul><li>triplets的数量是：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>K</mi></mrow><annotation encoding="application/x-tex">PK</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span></li></ul></li></ul><p>根据<a href="https://arxiv.org/abs/1703.07737">paper</a>实验结果，采用batch hard strategy通常可以产生更好的generalization performance。</p><h2 id="Online-Triplet-Mining-Implementation-in-Tensorflow">Online Triplet Mining Implementation in Tensorflow</h2><h3 id="Compute-the-distance-matrix">Compute the distance matrix</h3><p>由于triplet的选择最终取决于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(a,p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(a,n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>，因此我们可以先计算一个batch内样本两两之间的距离：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_pairwise_distances</span>(<span class="params">embeddings, squared=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the 2D matrix of distances between all the embeddings.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        embeddings: tensor of shape (batch_size, embed_dim)</span></span><br><span class="line"><span class="string">        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.</span></span><br><span class="line"><span class="string">                 If false, output is the pairwise euclidean distance matrix.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        pairwise_distances: tensor of shape (batch_size, batch_size)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Get the dot product between all embeddings</span></span><br><span class="line">    <span class="comment"># shape (batch_size, batch_size)</span></span><br><span class="line">    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.</span></span><br><span class="line">    <span class="comment"># This also provides more numerical stability (the diagonal of the result will be exactly 0).</span></span><br><span class="line">    <span class="comment"># shape (batch_size,)</span></span><br><span class="line">    square_norm = tf.diag_part(dot_product)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the pairwise distance matrix as we have:</span></span><br><span class="line">    <span class="comment"># ||a - b||^2 = ||a||^2  - 2 &lt;a, b&gt; + ||b||^2</span></span><br><span class="line">    <span class="comment"># shape (batch_size, batch_size)</span></span><br><span class="line">    distances = tf.expand_dims(square_norm, <span class="number">0</span>) - <span class="number">2.0</span> * dot_product + tf.expand_dims(square_norm, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Because of computation errors, some distances might be negative so we put everything &gt;= 0.0</span></span><br><span class="line">    distances = tf.maximum(distances, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> squared:</span><br><span class="line">        <span class="comment"># Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)</span></span><br><span class="line">        <span class="comment"># we need to add a small epsilon where distances == 0.0</span></span><br><span class="line">        mask = tf.to_float(tf.equal(distances, <span class="number">0.0</span>))</span><br><span class="line">        distances = distances + mask * <span class="number">1e-16</span></span><br><span class="line"></span><br><span class="line">        distances = tf.sqrt(distances)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Correct the epsilon added: set the distances on the mask to be exactly 0.0</span></span><br><span class="line">        distances = distances * (<span class="number">1.0</span> - mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> distances</span><br></pre></td></tr></table></figure><h3 id="Batch-all-strategy">Batch all strategy</h3><p>首先计算所有可能的triplets (总共 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>B</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">B^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span> 个)，然后选择确定valid triplets，最后计算triplet loss并仅在hard triplets和semi-hard triplets上计算average loss：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_all_triplet_loss</span>(<span class="params">labels, embeddings, margin, squared=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Build the triplet loss over a batch of embeddings.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    We generate all the valid triplets and average the loss over the positive ones.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        labels: labels of the batch, of size (batch_size,)</span></span><br><span class="line"><span class="string">        embeddings: tensor of shape (batch_size, embed_dim)</span></span><br><span class="line"><span class="string">        margin: margin for triplet loss</span></span><br><span class="line"><span class="string">        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.</span></span><br><span class="line"><span class="string">                 If false, output is the pairwise euclidean distance matrix.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        triplet_loss: scalar tensor containing the triplet loss</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Get the pairwise distance matrix</span></span><br><span class="line">    pairwise_dist = _pairwise_distances(embeddings, squared=squared)</span><br><span class="line"></span><br><span class="line">    anchor_positive_dist = tf.expand_dims(pairwise_dist, <span class="number">2</span>)</span><br><span class="line">    anchor_negative_dist = tf.expand_dims(pairwise_dist, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute a 3D tensor of size (batch_size, batch_size, batch_size)</span></span><br><span class="line">    <span class="comment"># triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k</span></span><br><span class="line">    <span class="comment"># Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)</span></span><br><span class="line">    <span class="comment"># and the 2nd (batch_size, 1, batch_size)</span></span><br><span class="line">    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put to zero the invalid triplets</span></span><br><span class="line">    <span class="comment"># (where label(a) != label(p) or label(n) == label(a) or a == p)</span></span><br><span class="line">    mask = _get_triplet_mask(labels)</span><br><span class="line">    mask = tf.to_float(mask)</span><br><span class="line">    triplet_loss = tf.multiply(mask, triplet_loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove negative losses (i.e. the easy triplets)</span></span><br><span class="line">    triplet_loss = tf.maximum(triplet_loss, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count number of positive triplets (where triplet_loss &gt; 0)</span></span><br><span class="line">    valid_triplets = tf.to_float(tf.greater(triplet_loss, <span class="number">1e-16</span>))</span><br><span class="line">    num_positive_triplets = tf.reduce_sum(valid_triplets)</span><br><span class="line">    num_valid_triplets = tf.reduce_sum(mask)</span><br><span class="line">    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + <span class="number">1e-16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get final mean triplet loss over the positive valid triplets</span></span><br><span class="line">    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + <span class="number">1e-16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> triplet_loss, fraction_positive_triplets</span><br></pre></td></tr></table></figure><h3 id="Batch-hard-strategy">Batch hard strategy</h3><p>为了计算hardest positive，我们首先计算valid mask(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo mathvariant="normal">≠</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">a \neq q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span> 但是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">a</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span></span></span></span> 是同类)，然后每个row中最大距离所对应的pair就是hardest positive。</p><p>为了计算hardest negative，我们需要找到每个row中的最小距离，因此在计算valid mask是不能将invalid mask设置为0，相反应该设置一个非常大的数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_hard_triplet_loss</span>(<span class="params">labels, embeddings, margin, squared=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Build the triplet loss over a batch of embeddings.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For each anchor, we get the hardest positive and hardest negative to form a triplet.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        labels: labels of the batch, of size (batch_size,)</span></span><br><span class="line"><span class="string">        embeddings: tensor of shape (batch_size, embed_dim)</span></span><br><span class="line"><span class="string">        margin: margin for triplet loss</span></span><br><span class="line"><span class="string">        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.</span></span><br><span class="line"><span class="string">                 If false, output is the pairwise euclidean distance matrix.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        triplet_loss: scalar tensor containing the triplet loss</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Get the pairwise distance matrix</span></span><br><span class="line">    pairwise_dist = _pairwise_distances(embeddings, squared=squared)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For each anchor, get the hardest positive</span></span><br><span class="line">    <span class="comment"># First, we need to get a mask for every valid positive (they should have same label)</span></span><br><span class="line">    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)</span><br><span class="line">    mask_anchor_positive = tf.to_float(mask_anchor_positive)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))</span></span><br><span class="line">    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shape (batch_size, 1)</span></span><br><span class="line">    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For each anchor, get the hardest negative</span></span><br><span class="line">    <span class="comment"># First, we need to get a mask for every valid negative (they should have different labels)</span></span><br><span class="line">    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)</span><br><span class="line">    mask_anchor_negative = tf.to_float(mask_anchor_negative)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We add the maximum value in each row to the invalid negatives (label(a) == label(n))</span></span><br><span class="line">    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (<span class="number">1.0</span> - mask_anchor_negative)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shape (batch_size,)</span></span><br><span class="line">    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Combine biggest d(a, p) and smallest d(a, n) into final triplet loss</span></span><br><span class="line">    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get final mean triplet loss</span></span><br><span class="line">    triplet_loss = tf.reduce_mean(triplet_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> triplet_loss</span><br></pre></td></tr></table></figure><h2 id="Resources">Resources</h2><ul><li><a href="https://omoindrot.github.io/triplet-loss">Triplet Loss and Online Triplet Mining in TensorFlow</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Loss Function </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
